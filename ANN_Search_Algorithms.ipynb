{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ANN_Search_Algorithms.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNn+QX7nKu/kmmsUPAZ7O6I",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ArshiaSali/Approximate-Nearest-Neighbor-Search/blob/main/ANN_Search_Algorithms.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bBBQeLmBoTSN"
      },
      "source": [
        "# **Approximate Nearest Neighbor Search Algorithms**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RX99AOPKossa"
      },
      "source": [
        "### **Dataset Used**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pv39sLYAs9Pz"
      },
      "source": [
        "The dataset contain users answering questions: An interaction is defined as a user answering a given question.\n",
        "\n",
        "The following datasets from the StackExchange network are available:\n",
        "\n",
        " - **CrossValidated:** From stats.stackexchange.com. Approximately 9000 users, 72000 questions, and 70000 answers.\n",
        "\n",
        " - **StackOverflow:** From stackoverflow.stackexchange.com. Approximately 1.3M users, 11M questions, and 18M answers.\n",
        "\n",
        "\n",
        "Here CrossValidated Dataset is considered for analysis.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hcVdFWbfoa83"
      },
      "source": [
        "### **Import packages and Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xqMBG4IYoiR6",
        "outputId": "362a7a9e-02a8-4b4d-9e0b-dbf32c3e41d2"
      },
      "source": [
        "import pickle\n",
        "!pip install lightfm\n",
        "from lightfm import LightFM\n",
        "from lightfm.datasets import fetch_stackexchange\n",
        "\n",
        "!pip install nmslib\n",
        "import nmslib\n",
        "\n",
        "!pip install faiss-cpu --no-cache\n",
        "import faiss\n",
        "\n",
        "!pip install annoy\n",
        "import annoy\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting lightfm\n",
            "  Downloading lightfm-1.16.tar.gz (310 kB)\n",
            "\u001b[?25l\r\u001b[K     |█                               | 10 kB 18.8 MB/s eta 0:00:01\r\u001b[K     |██▏                             | 20 kB 22.9 MB/s eta 0:00:01\r\u001b[K     |███▏                            | 30 kB 13.5 MB/s eta 0:00:01\r\u001b[K     |████▎                           | 40 kB 10.3 MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 51 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 61 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 71 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 81 kB 6.3 MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 92 kB 6.5 MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 102 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 112 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 122 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 133 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 143 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 153 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 163 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 174 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 184 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 194 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 204 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 215 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 225 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 235 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 245 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 256 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 266 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 276 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 286 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 296 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 307 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 310 kB 5.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from lightfm) (1.19.5)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from lightfm) (1.4.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from lightfm) (2.23.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from lightfm) (1.0.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->lightfm) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->lightfm) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->lightfm) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->lightfm) (2.10)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->lightfm) (3.0.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->lightfm) (1.1.0)\n",
            "Building wheels for collected packages: lightfm\n",
            "  Building wheel for lightfm (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for lightfm: filename=lightfm-1.16-cp37-cp37m-linux_x86_64.whl size=705355 sha256=f5852850d96d97ada19ba3486a7a0e646f6ff001cfca620d4770ba4377abaf12\n",
            "  Stored in directory: /root/.cache/pip/wheels/f8/56/28/5772a3bd3413d65f03aa452190b00898b680b10028a1021914\n",
            "Successfully built lightfm\n",
            "Installing collected packages: lightfm\n",
            "Successfully installed lightfm-1.16\n",
            "Collecting nmslib\n",
            "  Downloading nmslib-2.1.1-cp37-cp37m-manylinux2010_x86_64.whl (13.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 13.5 MB 1.4 MB/s \n",
            "\u001b[?25hCollecting pybind11<2.6.2\n",
            "  Downloading pybind11-2.6.1-py2.py3-none-any.whl (188 kB)\n",
            "\u001b[K     |████████████████████████████████| 188 kB 51.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (from nmslib) (5.4.8)\n",
            "Requirement already satisfied: numpy>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from nmslib) (1.19.5)\n",
            "Installing collected packages: pybind11, nmslib\n",
            "Successfully installed nmslib-2.1.1 pybind11-2.6.1\n",
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.7.1.post2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 8.4 MB 4.5 MB/s \n",
            "\u001b[?25hInstalling collected packages: faiss-cpu\n",
            "Successfully installed faiss-cpu-1.7.1.post2\n",
            "Collecting annoy\n",
            "  Downloading annoy-1.17.0.tar.gz (646 kB)\n",
            "\u001b[K     |████████████████████████████████| 646 kB 5.3 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: annoy\n",
            "  Building wheel for annoy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for annoy: filename=annoy-1.17.0-cp37-cp37m-linux_x86_64.whl size=391687 sha256=668ae0d578869fa71a3b069f55c57b7e42782bae22acbec443ad340a6d98c2bc\n",
            "  Stored in directory: /root/.cache/pip/wheels/4f/e8/1e/7cc9ebbfa87a3b9f8ba79408d4d31831d67eea918b679a4c07\n",
            "Successfully built annoy\n",
            "Installing collected packages: annoy\n",
            "Successfully installed annoy-1.17.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ANNZGmv-hc1M",
        "outputId": "901652e7-86ab-44c6-f5f7-2136918ab717"
      },
      "source": [
        "!pip install stackapi\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting stackapi\n",
            "  Downloading StackAPI-0.2.0.tar.gz (5.6 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from stackapi) (2.23.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from stackapi) (1.15.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->stackapi) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->stackapi) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->stackapi) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->stackapi) (1.24.3)\n",
            "Building wheels for collected packages: stackapi\n",
            "  Building wheel for stackapi (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for stackapi: filename=StackAPI-0.2.0-py3-none-any.whl size=5856 sha256=7f84a460f4ab99841796927032ec26f9f31060347d101a9e9bbee307db9f8b7c\n",
            "  Stored in directory: /root/.cache/pip/wheels/ec/db/60/df42a65853e3581c26a2fbb2012a228cb8e267369a3b9ca44d\n",
            "Successfully built stackapi\n",
            "Installing collected packages: stackapi\n",
            "Successfully installed stackapi-0.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1eaDwpT0gmM6"
      },
      "source": [
        "### **Import Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pb6OcJByjeZN"
      },
      "source": [
        "stackexchange = fetch_stackexchange('crossvalidated')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nPmBmVy8jeco"
      },
      "source": [
        "train = stackexchange['train']\n",
        "test = stackexchange['test']\n",
        "\n",
        "model = LightFM(learning_rate=0.05, loss='warp', no_components=64, item_alpha=0.001)\n",
        "model.fit_partial(train, item_features=stackexchange['item_features'], epochs=20 )\n",
        "\n",
        "item_vectors = stackexchange['item_features'] * model.item_embeddings\n",
        "\n",
        "from termcolor import cprint\n",
        "print_red_on_cyan = lambda x: cprint(x, 'yellow', 'on_grey')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dwlQOLC8cpjO",
        "outputId": "91893392-e555-4f91-9569-fe4272ab152b"
      },
      "source": [
        "stackexchange['item_feature_labels']"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['question_id:0', 'question_id:1', 'question_id:2', ...,\n",
              "       'question_id:72357', 'question_id:72358', 'question_id:72359'],\n",
              "      dtype='<U17')"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "omv9j7dPpl31"
      },
      "source": [
        "### **Pickle the Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QDeiT3ILdc1Z"
      },
      "source": [
        "with open('stack_exchange.pickle', 'wb') as f:\n",
        "    pickle.dump({\"name\": stackexchange['item_feature_labels'], \"vector\": item_vectors}, f)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQUlY74Fpp_z"
      },
      "source": [
        "### **Function to Load Pickled Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U1pAr6J-dc4F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8852a5e-2080-4e73-f82b-16f9b2ebed67"
      },
      "source": [
        "def load_data():\n",
        "    with open('stack_exchange.pickle', 'rb') as f:\n",
        "        data = pickle.load(f)\n",
        "    return data\n",
        "\n",
        "data = load_data()\n",
        "data"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'name': array(['question_id:0', 'question_id:1', 'question_id:2', ...,\n",
              "        'question_id:72357', 'question_id:72358', 'question_id:72359'],\n",
              "       dtype='<U17'),\n",
              " 'vector': array([[ 0.05906702,  0.02375606,  0.07332865, ..., -0.01101989,\n",
              "         -0.0683156 ,  0.08668615],\n",
              "        [ 0.13899006,  0.11184364,  0.05210828, ..., -0.10418914,\n",
              "         -0.16255663,  0.00553737],\n",
              "        [-0.01872136, -0.13028923,  0.11398011, ...,  0.05952855,\n",
              "         -0.16864328, -0.07374636],\n",
              "        ...,\n",
              "        [-0.0119923 ,  0.02600207, -0.02288014, ..., -0.00684505,\n",
              "          0.02163115, -0.02398989],\n",
              "        [ 0.00402075,  0.00426441,  0.00309685, ..., -0.01254157,\n",
              "          0.00961653, -0.00694339],\n",
              "        [-0.0084562 , -0.00758704,  0.01070431, ...,  0.00627414,\n",
              "         -0.01246643,  0.01460843]], dtype=float32)}"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OmAeGJozCmLT"
      },
      "source": [
        "##**Locality Sensitive Hashing**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vswHGYPwtoHp"
      },
      "source": [
        "**LSH** refers to a family of functions (known as LSH families) to hash data points into buckets so that data points near each other are located in the same buckets with high probability, while data points far from each other are likely to be in different buckets. \n",
        "\n",
        "This makes it easier to identify observations with various degrees of similarity.\n",
        "\n",
        "LSH has many applications, including:\n",
        "\n",
        "\n",
        "**Near-duplicate detection:**\n",
        "\n",
        " LSH is commonly used to deduplicate large quantities of documents, webpages, and other files.\n",
        "\n",
        "**Genome-wide association study:**\n",
        "\n",
        " Biologists often use LSH to identify similar gene expressions in genome databases.\n",
        "\n",
        "**Large-scale image search:** \n",
        "\n",
        "Google used LSH along with PageRank to build their image search technology VisualRank.\n",
        "\n",
        "**Audio/video fingerprinting:** \n",
        "\n",
        "In multimedia technologies, LSH is widely used as a fingerprinting technique A/V data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bSNphuMeDL4l"
      },
      "source": [
        "###**Index Class**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GlUBup7hgAv6"
      },
      "source": [
        "#LSH\n",
        "class LSHIndex():\n",
        "    def __init__(self, vectors, labels):\n",
        "        self.dimension = vectors.shape[1]\n",
        "        self.vectors = vectors.astype('float32')\n",
        "        self.labels = labels    \n",
        "   \n",
        "    def build(self, num_bits=8):\n",
        "        self.index = faiss.IndexLSH(self.dimension, num_bits)\n",
        "        self.index.add(self.vectors)\n",
        "        \n",
        "    def query(self, vectors, k=10):\n",
        "        distances, indices = self.index.search(vectors, k) \n",
        "        # I expect only query on one vector thus the slice\n",
        "        return [self.labels[i] for i in indices[0]]"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YgkAusFyDO_Z"
      },
      "source": [
        "###**Build Index**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ilbzsRRwgAyv"
      },
      "source": [
        "index = LSHIndex(data[\"vector\"], data[\"name\"])\n",
        "index.build()"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QBbx-YBHX7ER"
      },
      "source": [
        "###**Query for Similar Questions**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qXHJJLOxDg3u"
      },
      "source": [
        "Here, we are querying the index to return questions that are similar to the Question Id - 90."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_j6WXdbngA1t",
        "outputId": "2eae6653-f141-4413-b799-6b414d14a9a3"
      },
      "source": [
        "question_vector, question_id = data['vector'][90:91], data['name'][90]\n",
        "simlar_question_ids = '\\n* '.join(index.query(question_vector))\n",
        "print(f\"The most similar questions to {question_id} are:\\n* {simlar_question_ids}\")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The most similar questions to question_id:90 are:\n",
            "* question_id:155\n",
            "* question_id:189\n",
            "* question_id:172\n",
            "* question_id:51\n",
            "* question_id:6\n",
            "* question_id:240\n",
            "* question_id:253\n",
            "* question_id:323\n",
            "* question_id:90\n",
            "* question_id:22\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kbO3Sei0JaVG"
      },
      "source": [
        "### **Visualize Similar Questions**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "088Er_mg4lCq",
        "outputId": "2d122ceb-8a98-4cbb-c10a-220bcfb661da"
      },
      "source": [
        "from stackapi import StackAPI\n",
        "QUESTION_IDS = [\n",
        "514\n",
        ",90\n",
        ",594] \n",
        "SITENAME = 'stackoverflow'\n",
        "SITE = StackAPI(SITENAME)\n",
        "\n",
        "for qn in QUESTION_IDS:\n",
        "  question = SITE.fetch('questions/{ids}', ids=[qn], filter='withbody')\n",
        "  print_red_on_cyan(\"Question : \" + question['items'][0]['title'])\n",
        "  print()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[40m\u001b[33mQuestion : Frequent SystemExit in Ruby when making HTTP calls\u001b[0m\n",
            "\n",
            "\u001b[40m\u001b[33mQuestion : How do you branch and merge with TortoiseSVN?\u001b[0m\n",
            "\n",
            "\u001b[40m\u001b[33mQuestion : cx_Oracle: How do I iterate over a result set?\u001b[0m\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jm3f8OmqQxGI"
      },
      "source": [
        "### **Inference** \n",
        "We can see the questions similar to Question Id 9 - **How do you branch and merge with TortoiseSVN?** using Locality Sensitive Hashing Technique. \n",
        "\n",
        "LSH refers to functions to hash data points into buckets so that data points near each other are located in the same buckets with high probability, while data points far from each other are likely to be in different buckets.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2pRDEmLM6U7F"
      },
      "source": [
        "##**Exhaustive Search**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pqnqDr4A6H8t"
      },
      "source": [
        "**Exhaustive search**- Comparing each point to every other point, which will require Linear query time (the size of the dataset).\n",
        "\n",
        "The only available method for guaranteed retrieval of the exact nearest neighbor is exhaustive search (due to the curse of dimensionality.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QFJG3O0g8caH"
      },
      "source": [
        "###**Index Class**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KA5RJSvrdc6w"
      },
      "source": [
        "#Exhaustive Search\n",
        "class ExactIndex():\n",
        "    def __init__(self, vectors, labels):\n",
        "        self.dimension = vectors.shape[1]\n",
        "        self.vectors = vectors.astype('float32')\n",
        "        self.labels = labels    \n",
        "   \n",
        "    def build(self):\n",
        "        self.index = faiss.IndexFlatL2(self.dimension,)\n",
        "        self.index.add(self.vectors)\n",
        "        \n",
        "    def query(self, vectors, k=10):\n",
        "        distances, indices = self.index.search(vectors, k) \n",
        "        # I expect only query on one vector thus the slice\n",
        "        return [self.labels[i] for i in indices[0]]"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J4ypIkVe8dc3"
      },
      "source": [
        "###**Build Index**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fESFFoTmdc9L"
      },
      "source": [
        "index = ExactIndex(data[\"vector\"], data[\"name\"])\n",
        "index.build()"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FFZBkKIXBaWz"
      },
      "source": [
        "###**Query for Similar Questions**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EnqVQNbZKh_7"
      },
      "source": [
        "Here, we are querying the index to return questions that are similar to the Question Id - 90."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3OtSvi-WndUR",
        "outputId": "f933f6dd-3e0a-45a8-b297-fa4269e136ce"
      },
      "source": [
        "question_vector, question_id = data['vector'][90:91], data['name'][90]\n",
        "simlar_question_ids = '\\n* '.join(index.query(question_vector))\n",
        "print(f\"The most similar questions to {question_id} are:\\n* {simlar_question_ids}\")"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The most similar questions to question_id:90 are:\n",
            "* question_id:90\n",
            "* question_id:296\n",
            "* question_id:3129\n",
            "* question_id:916\n",
            "* question_id:3356\n",
            "* question_id:6\n",
            "* question_id:569\n",
            "* question_id:433\n",
            "* question_id:3050\n",
            "* question_id:444\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kQuJaenIJZFL"
      },
      "source": [
        "### **Visualize Similar Questions**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vED5JcApn3oa",
        "outputId": "1cca92ca-3708-4c97-aed7-6ddd85a295b5"
      },
      "source": [
        "from stackapi import StackAPI\n",
        "QUESTION_IDS = [90,5328,17681,6440] # For example\n",
        "SITENAME = 'stackoverflow'\n",
        "SITE = StackAPI(SITENAME)\n",
        "\n",
        "for qn in QUESTION_IDS:\n",
        "  question = SITE.fetch('questions/{ids}', ids=[qn], filter='withbody')\n",
        "  print_red_on_cyan(\"Question : \" + question['items'][0]['title'])\n",
        "  print()\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[40m\u001b[33mQuestion : How do you branch and merge with TortoiseSVN?\u001b[0m\n",
            "\n",
            "\u001b[40m\u001b[33mQuestion : Why can&#39;t I use a try block around my super() call?\u001b[0m\n",
            "\n",
            "\u001b[40m\u001b[33mQuestion : WebSVN with VisualSVN Server, anyone gotten authentication to work?\u001b[0m\n",
            "\n",
            "\u001b[40m\u001b[33mQuestion : .NET 3.5 Redistributable -- 200 MB? Other options?\u001b[0m\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P1g9K_GxQnTh"
      },
      "source": [
        "### **Inference** \n",
        "We can see the questions similar to Question Id 9 - **How do you branch and merge with TortoiseSVN?** using Exhaustive Search Technique. \n",
        "\n",
        "Exhaustive search- Comparing each point to every other point, which will require Linear query time (the size of the dataset).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-DwxM5GcDmje"
      },
      "source": [
        "##**Product Quantization**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SvHg-bsB7Shi"
      },
      "source": [
        "**Product quantization** is an effective vector quantization\n",
        "approach to compactly encode high-dimensional vectors\n",
        "for fast approximate nearest neighbor (ANN) search. \n",
        "\n",
        "The\n",
        "essence of product quantization is to decompose the original high-dimensional space into the Cartesian product of\n",
        "a finite number of low-dimensional subspaces that are then\n",
        "quantized separately.\n",
        "\n",
        "Optimal space decomposition is important for the performance of ANN search, but still remains unaddressed. In this paper, we optimize product quantization by minimizing quantization distortions w.r.t. the space decomposition and the quantization codebooks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u5hWD3QWX4Us"
      },
      "source": [
        "###**Index Class**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "00rzxRrSD2O_"
      },
      "source": [
        "Create the index class where we can control the subvector_size, number_of_partitions and search_in_x_partitions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l9xuZ-hqgA4a"
      },
      "source": [
        "#Product Quantization\n",
        "class IVPQIndex():\n",
        "    def __init__(self, vectors, labels):\n",
        "        self.dimension = vectors.shape[1]\n",
        "        self.vectors = vectors.astype('float32')\n",
        "        self.labels = labels    \n",
        "    def build(self, number_of_partition=8, search_in_x_partitions=2, subvector_size=8):\n",
        "        quantizer = faiss.IndexFlatL2(self.dimension)\n",
        "        self.index = faiss.IndexIVFPQ(quantizer, \n",
        "                                      self.dimension, \n",
        "                                      number_of_partition, \n",
        "                                      search_in_x_partitions, \n",
        "                                      subvector_size)\n",
        "        self.index.train(self.vectors)\n",
        "        self.index.add(self.vectors)\n",
        "        \n",
        "    def query(self, vectors, k=10):\n",
        "        distances, indices = self.index.search(vectors, k) \n",
        "        # I expect only query on one vector thus the slice\n",
        "        return [self.labels[i] for i in indices[0]]"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5uVx8TW2EFCv"
      },
      "source": [
        "###**Build Index**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nKck1O7igA7E"
      },
      "source": [
        "index = IVPQIndex(data[\"vector\"], data[\"name\"])\n",
        "index.build()"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v9Hl_4CzX0KU"
      },
      "source": [
        "### **Query for Similar Questions**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iwuTlenWKjEZ"
      },
      "source": [
        "Here, we are querying the index to return questions that are similar to the Question Id - 90."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mDmzdVc_7gM6",
        "outputId": "9740c52e-761c-452b-ed57-bbdc8e70d8ee"
      },
      "source": [
        "question_vector, question_id = data['vector'][90:91], data['name'][90]\n",
        "simlar_question_ids = '\\n* '.join(index.query(question_vector))\n",
        "print(f\"The most similar questions to {question_id} are:\\n* {simlar_question_ids}\")"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The most similar questions to question_id:90 are:\n",
            "* question_id:90\n",
            "* question_id:3678\n",
            "* question_id:5508\n",
            "* question_id:1582\n",
            "* question_id:6280\n",
            "* question_id:569\n",
            "* question_id:306\n",
            "* question_id:1886\n",
            "* question_id:2300\n",
            "* question_id:6440\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Pj8XtDzJPRc"
      },
      "source": [
        "### **Visualize Similar Questions**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S-YrGIm17gM6",
        "outputId": "bcf3ea2f-1e96-4f2b-c0b3-3872294a6e65"
      },
      "source": [
        "from stackapi import StackAPI\n",
        "QUESTION_IDS = [9\n",
        ",6\n",
        ",90\n",
        ",5328\n",
        ",9136\n",
        "] \n",
        "SITENAME = 'stackoverflow'\n",
        "SITE = StackAPI(SITENAME)\n",
        "\n",
        "for qn in QUESTION_IDS:\n",
        "  question = SITE.fetch('questions/{ids}', ids=[qn], filter='withbody')\n",
        "  print_red_on_cyan(\"Question : \" + question['items'][0]['title'])\n",
        "  print()\n"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[40m\u001b[33mQuestion : How do I calculate someone&#39;s age based on a DateTime type birthday?\u001b[0m\n",
            "\n",
            "\u001b[40m\u001b[33mQuestion : Why did the width collapse in the percentage width child element in an absolutely positioned parent on Internet Explorer 7?\u001b[0m\n",
            "\n",
            "\u001b[40m\u001b[33mQuestion : How do you branch and merge with TortoiseSVN?\u001b[0m\n",
            "\n",
            "\u001b[40m\u001b[33mQuestion : Why can&#39;t I use a try block around my super() call?\u001b[0m\n",
            "\n",
            "\u001b[40m\u001b[33mQuestion : Enterprise Library CacheFactory.GetCacheManager Throws Null Ref\u001b[0m\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7r1AbU9tQZvq"
      },
      "source": [
        "### **Inference** \n",
        "We can see the questions similar to Question Id 9 - **How do you branch and merge with TortoiseSVN?** using Product Quantization Technique. \n",
        "\n",
        "Product quantization is an effective vector quantization approach to compactly encode high-dimensional vectors for fast approximate nearest neighbor (ANN) search."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uk_ZmRRuEdfy"
      },
      "source": [
        "##**Trees and Graphs**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DdrZVSyF80pA"
      },
      "source": [
        "**Tree-based algorithms** are one of the most common strategies when it comes to ANN. They construct forests (collection of trees) as their data structure by **splitting the dataset into subsets**.\n",
        "\n",
        "\n",
        "One of the most prominent solutions out there is **Annoy**, which uses trees (more accurately forests) to enable Spotify’ music recommendations. \n",
        "\n",
        "In Annoy, in order to construct the index we create a forest (aka many trees) Each tree is constructed in the following way, we pick two points at random and split the space into two by their hyperplane, we keep splitting into the subspaces recursively until the points associated with a node is small enough.\n",
        "\n",
        "\n",
        "In order to search the constructed index, the forest is traversed in order to obtain a set of candidate points from which the closest to the query point is returned.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uuET0InCXtw8"
      },
      "source": [
        "###**Index class**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "igAtTGHL9HMC"
      },
      "source": [
        "**Annoy Usage**\n",
        "\n",
        "\n",
        "We use annoy library. Most of the logic is in the build method (index creation), where the accuracy-performance tradeoff is controlled by:\n",
        "\n",
        "**number_of_trees** — the number of binary trees , a larger value will give more accurate results, but larger indexes.\n",
        "\n",
        "**search_k** — the number of binary trees we search for each point, a larger value will give more accurate results, but will take a longer time to return."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MofDAyhHjA3C"
      },
      "source": [
        "#Trees\n",
        "class AnnoyIndex():\n",
        "    def __init__(self, vectors, labels):\n",
        "        self.dimention = vectors.shape[1]\n",
        "        self.vectors = vectors.astype('float32')\n",
        "        self.labels = labels\n",
        "\n",
        "\n",
        "    def build(self, number_of_trees=5):\n",
        "        self.index = annoy.AnnoyIndex(self.dimention)\n",
        "        for i, vec in enumerate(self.vectors):\n",
        "            self.index.add_item(i, vec.tolist())\n",
        "        self.index.build(number_of_trees)\n",
        "        \n",
        "    def query(self, vector, k=10):\n",
        "        indices = self.index.get_nns_by_vector(vector.tolist(), k)\n",
        "        return [self.labels[i] for i in indices]"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6eS4OmiCFKKr"
      },
      "source": [
        "###**Build Index**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nsen0fOijA5u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "51f50ebe-bc6a-4ad9-f01e-3b3e3acabab7"
      },
      "source": [
        "index = AnnoyIndex(data[\"vector\"], data[\"name\"])\n",
        "index.build()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:10: FutureWarning: The default argument for metric will be removed in future version of Annoy. Please pass metric='angular' explicitly.\n",
            "  # Remove the CWD from sys.path while we load stuff.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IZKYqtx4W6-Q"
      },
      "source": [
        "###**Query for Similar Questions**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZrZhGZigKj9f"
      },
      "source": [
        "Here, we are querying the index to return questions that are similar to the Question Id - 90."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XKZxCNHW9g80",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40d796f0-eb46-4bca-e981-07eebd2a01b1"
      },
      "source": [
        "question_vector, question_id = data['vector'][90], data['name'][90]\n",
        "simlar_question_ids = '\\n* '.join(index.query(question_vector))\n",
        "print(f\"The most similar questions to {question_id} are:\\n* {simlar_question_ids}\")"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The most similar questions to question_id:90 are:\n",
            "* question_id:90\n",
            "* question_id:3129\n",
            "* question_id:102\n",
            "* question_id:450\n",
            "* question_id:1947\n",
            "* question_id:3451\n",
            "* question_id:3789\n",
            "* question_id:3366\n",
            "* question_id:253\n",
            "* question_id:49022\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pzbSoYf4JVR5"
      },
      "source": [
        "### **Visualize Similar Questions**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K08n9nnc9g88",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "32f1fefb-81ec-4d41-f101-5331a9feeed7"
      },
      "source": [
        "from stackapi import StackAPI\n",
        "QUESTION_IDS = [90\n",
        ",5328\n",
        ",13\n",
        ",34] \n",
        "SITENAME = 'stackoverflow'\n",
        "SITE = StackAPI(SITENAME)\n",
        "\n",
        "for qn in QUESTION_IDS:\n",
        "  question = SITE.fetch('questions/{ids}', ids=[qn], filter='withbody')\n",
        "  print_red_on_cyan(\"Question : \" + question['items'][0]['title'])\n",
        "  print()\n"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[40m\u001b[33mQuestion : How do you branch and merge with TortoiseSVN?\u001b[0m\n",
            "\n",
            "\u001b[40m\u001b[33mQuestion : Why can&#39;t I use a try block around my super() call?\u001b[0m\n",
            "\n",
            "\u001b[40m\u001b[33mQuestion : Determine a user&#39;s timezone\u001b[0m\n",
            "\n",
            "\u001b[40m\u001b[33mQuestion : How to unload a ByteArray using Actionscript 3?\u001b[0m\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZtUrHSYbQRw0"
      },
      "source": [
        "### **Inference** \n",
        "We can see the questions similar to Question Id 9 - **How do you branch and merge with TortoiseSVN?** using Trees and Graphs Technique. \n",
        "\n",
        "Tree-based algorithms are one of the most common strategies when it comes to ANN. They construct forests (collection of trees) as their data structure by splitting the dataset into subsets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ud8U5V0YFZmA"
      },
      "source": [
        "##**HNSW**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5-CkFYH036Iw"
      },
      "source": [
        "An **HNSW index** consists of navigable small world graphs in a hierarchy. Each document in the index is represented by a single graph node. Each node has an array of levels, from level 0 to n. The number of levels is constant during the lifetime of the node and is drawn randomly when the node is created. All nodes have at least level 0. At each level there is a link array which contains the document ids of the nodes it is connected to at that level."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PPYZAyxuFcPG"
      },
      "source": [
        "###**Index class**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-v56R692jBBE"
      },
      "source": [
        "#hsnw\n",
        "class NMSLIBIndex():\n",
        "    def __init__(self, vectors, labels):\n",
        "        self.dimention = vectors.shape[1]\n",
        "        self.vectors = vectors.astype('float32')\n",
        "        self.labels = labels\n",
        "    def build(self):\n",
        "        self.index = nmslib.init(method='hnsw', space='cosinesimil')\n",
        "        self.index.addDataPointBatch(self.vectors)\n",
        "        self.index.createIndex({'post': 2})\n",
        "        \n",
        "    def query(self, vector, k=10):\n",
        "        indices = self.index.knnQuery(vector, k=k)\n",
        "        return [self.labels[i] for i in indices[0]]"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J7R6HjiBFijg"
      },
      "source": [
        "###**Build Index**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4878fTCMkXFk"
      },
      "source": [
        "index = NMSLIBIndex(data[\"vector\"], data[\"name\"])\n",
        "index.build()"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qP9uMzRmWyK_"
      },
      "source": [
        "###**Query for Similar Questions**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cMqxjmK_KlL8"
      },
      "source": [
        "Here, we are querying the index to return questions that are similar to the Question Id - 90."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6zAatEtX_hf-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c740ff7-96e6-4494-df31-fe2ad7547ef9"
      },
      "source": [
        "question_vector, question_id = data['vector'][90], data['name'][90]\n",
        "simlar_question_ids = '\\n* '.join(index.query(question_vector))\n",
        "print(f\"The most similar questions to {question_id} are:\\n* {simlar_question_ids}\")"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The most similar questions to question_id:90 are:\n",
            "* question_id:90\n",
            "* question_id:296\n",
            "* question_id:14\n",
            "* question_id:433\n",
            "* question_id:3129\n",
            "* question_id:1394\n",
            "* question_id:22\n",
            "* question_id:102\n",
            "* question_id:762\n",
            "* question_id:6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M4VfYAZOJW_V"
      },
      "source": [
        "### **Visualize Similar Questions**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G2JQ66Gd_hgE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "28e92ef0-ccb2-47af-e368-d9eab41167ca"
      },
      "source": [
        "from stackapi import StackAPI\n",
        "QUESTION_IDS = [\n",
        "                90,6440,5328,17681\n",
        "] \n",
        "SITENAME = 'stackoverflow'\n",
        "SITE = StackAPI(SITENAME)\n",
        "\n",
        "for qn in QUESTION_IDS:\n",
        "  question = SITE.fetch('questions/{ids}', ids=[qn], filter='withbody')\n",
        "  print_red_on_cyan(\"Question : \" + question['items'][0]['title'])\n",
        "  print()\n"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[40m\u001b[33mQuestion : How do you branch and merge with TortoiseSVN?\u001b[0m\n",
            "\n",
            "\u001b[40m\u001b[33mQuestion : .NET 3.5 Redistributable -- 200 MB? Other options?\u001b[0m\n",
            "\n",
            "\u001b[40m\u001b[33mQuestion : Why can&#39;t I use a try block around my super() call?\u001b[0m\n",
            "\n",
            "\u001b[40m\u001b[33mQuestion : WebSVN with VisualSVN Server, anyone gotten authentication to work?\u001b[0m\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yH9JR-n7M69H"
      },
      "source": [
        "### **Inference** \n",
        "We can see the questions similar to Question Id 9 - **How do you branch and merge with TortoiseSVN?** using HNSW Technique. \n",
        "\n",
        "An HNSW index consists of navigable small world graphs in a hierarchy. Each document in the index is represented by a single graph node. Each node has an array of levels, from level 0 to n. The number of levels is constant during the lifetime of the node and is drawn randomly when the node is created. All nodes have at least level 0. At each level there is a link array which contains the document ids of the nodes it is connected to at that level."
      ]
    }
  ]
}